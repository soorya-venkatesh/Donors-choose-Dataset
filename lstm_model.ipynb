{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense,Embedding,Flatten,concatenate,MaxPooling1D,Dropout,Input,Conv1D,BatchNormalization,Reshape,LSTM,LeakyReLU\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#readding data\n",
    "df=pd.read_csv(\"pre_processed.csv\")\n",
    "df['teacher_prefix'].fillna(value=\"Mrs.\", inplace=True)\n",
    "y = df['project_is_approved'].values \n",
    "x = df.drop(['project_is_approved'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "x_train1,x_test,y_train1,y_test=train_test_split(x,y,test_size=0.2,stratify=y)\n",
    "x_train,x_cv,y_train,y_cv=train_test_split(x_train1,y_train1,test_size=0.2,stratify=y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#allocating seperate variables for features in the data\n",
    "xtr_essay= x_train[\"essay\"].values\n",
    "xtr_state= x_train[\"school_state\"].values\n",
    "xtr_grade= x_train[\"project_grade_category\"].values\n",
    "xtr_cat= x_train[\"project_subject_categories\"].values\n",
    "xtr_subcat= x_train[\"project_subject_subcategories\"].values\n",
    "xtr_Tprefix= x_train[\"teacher_prefix\"].values\n",
    "xtr_num= x_train[['teacher_number_of_previously_posted_projects', 'contains_digit', 'price','quantity']].copy().values\n",
    "\n",
    "xte_essay= x_test[\"essay\"].values\n",
    "xte_state= x_test[\"school_state\"].values\n",
    "xte_grade= x_test[\"project_grade_category\"].values\n",
    "xte_cat= x_test[\"project_subject_categories\"].values\n",
    "xte_subcat= x_test[\"project_subject_subcategories\"].values\n",
    "xte_Tprefix= x_test[\"teacher_prefix\"].values\n",
    "xte_num= x_test[['teacher_number_of_previously_posted_projects', 'contains_digit', 'price','quantity']].copy().values\n",
    "\n",
    "xcv_essay= x_cv[\"essay\"].values\n",
    "xcv_state= x_cv[\"school_state\"].values\n",
    "xcv_grade= x_cv[\"project_grade_category\"].values\n",
    "xcv_cat= x_cv[\"project_subject_categories\"].values\n",
    "xcv_subcat= x_cv[\"project_subject_subcategories\"].values\n",
    "xcv_Tprefix= x_cv[\"teacher_prefix\"].values\n",
    "xcv_num= x_cv[['teacher_number_of_previously_posted_projects', 'contains_digit', 'price','quantity']].copy().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalising numerical values\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "xtr_num=normalizer.fit_transform(xtr_num)\n",
    "xte_num=normalizer.transform(xte_num)\n",
    "xcv_num=normalizer.transform(xcv_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenising categorical variables\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(xtr_state)\n",
    "xtr_state = tokenizer.texts_to_sequences(xtr_state)\n",
    "xcv_state = tokenizer.texts_to_sequences(xcv_state)\n",
    "xte_state = tokenizer.texts_to_sequences(xte_state)\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(xtr_grade)\n",
    "xtr_grade = tokenizer.texts_to_sequences(xtr_grade)\n",
    "xcv_grade = tokenizer.texts_to_sequences(xcv_grade)\n",
    "xte_grade = tokenizer.texts_to_sequences(xte_grade)\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',oov_token = True)\n",
    "tokenizer.fit_on_texts(xtr_cat)\n",
    "xtr_cat = tokenizer.texts_to_sequences(xtr_cat)\n",
    "xcv_cat = tokenizer.texts_to_sequences(xcv_cat)\n",
    "xte_cat = tokenizer.texts_to_sequences(xte_cat)\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n',oov_token = True)\n",
    "tokenizer.fit_on_texts(xtr_subcat)\n",
    "xtr_subcat = tokenizer.texts_to_sequences(xtr_subcat)\n",
    "xcv_subcat = tokenizer.texts_to_sequences(xcv_subcat)\n",
    "xte_subcat = tokenizer.texts_to_sequences(xte_subcat)\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(xtr_Tprefix)\n",
    "xtr_Tprefix = tokenizer.texts_to_sequences(xtr_Tprefix)\n",
    "xcv_Tprefix = tokenizer.texts_to_sequences(xcv_Tprefix)\n",
    "xte_Tprefix = tokenizer.texts_to_sequences(xte_Tprefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to numpy arrays\n",
    "xtr_state=np.array(xtr_state)\n",
    "xcv_state=np.array(xcv_state)\n",
    "xte_state=np.array(xte_state)\n",
    "\n",
    "xtr_grade=np.array(xtr_grade)\n",
    "xcv_grade=np.array(xcv_grade)\n",
    "xte_grade=np.array(xte_grade)\n",
    "\n",
    "xtr_cat=np.array(xtr_cat)\n",
    "xcv_cat=np.array(xcv_cat)\n",
    "xte_cat=np.array(xte_cat)\n",
    "\n",
    "xtr_subcat=np.array(xtr_subcat)\n",
    "xcv_subcat=np.array(xcv_subcat)\n",
    "#xcv_subcat = np.array(list(x for x in xcv_subcat))\n",
    "xte_subcat=np.array(xte_subcat)\n",
    "#xte_subcat = np.array(list(x for x in xte_subcat))\n",
    "\n",
    "xtr_Tprefix=np.array(xtr_Tprefix)\n",
    "xcv_Tprefix=np.array(xcv_Tprefix)\n",
    "xte_Tprefix=np.array(xte_Tprefix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@davidheffernan_99410/an-introduction-to-using-categorical-embeddings-ee686ed7e7f9\n",
    "cat_vars = [\"teacher_prefix\",\"school_state\",\"project_grade_category\",\"project_subject_categories\",\"project_subject_subcategories\"]\n",
    "cat_sizes = {}\n",
    "cat_embsizes = {}\n",
    "for cat in cat_vars:\n",
    "    cat_sizes[cat] = x_train[cat].nunique()\n",
    "    cat_embsizes[cat] = min(50, cat_sizes[cat]//2+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenising essay feature\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(xtr_essay)\n",
    "xtr_essay = tokenizer.texts_to_sequences(xtr_essay)\n",
    "xcv_essay = tokenizer.texts_to_sequences(xcv_essay)\n",
    "xte_essay = tokenizer.texts_to_sequences(xte_essay)\n",
    "\n",
    "xtr_essay = pad_sequences(xtr_essay, maxlen=300, padding='post')\n",
    "xcv_essay=pad_sequences(xcv_essay, maxlen=300, padding='post')\n",
    "xte_essay = pad_sequences(xte_essay, maxlen=300, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding target variables\n",
    "y_train=tf.keras.utils.to_categorical(y_train, 2)\n",
    "y_cv=tf.keras.utils.to_categorical(y_cv, 2)\n",
    "y_test=tf.keras.utils.to_categorical(y_test, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\ana\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2f0790202846a2805ff5345024e52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#LOADING PRETRAINIED GLOVE MODEL\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.300d.txt',encoding=\"utf8\")\n",
    "for line in tqdm_notebook(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S:\\ana\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6010deb4e47e4b379c16b26b27a1af8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=47272.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#CREATING EMBEDDED MATRIX\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in tqdm_notebook(tokenizer.word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILDING MODEL ARCHITECTURE\n",
    "ins = []\n",
    "concat = []\n",
    "tf.keras.backend.clear_session()\n",
    "inp =  Input(shape=(300,))\n",
    "ins.append(inp)\n",
    "e=Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=300, trainable=False)(inp)\n",
    "l=LSTM(150,kernel_initializer='glorot_normal',recurrent_dropout=0.4,activation='relu',return_sequences=True)(e)\n",
    "f1=Flatten()(l)\n",
    "concat.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in cat_vars:\n",
    "    x = Input((1,), name=cat)\n",
    "    ins.append(x)\n",
    "    x = Embedding(cat_sizes[cat]+2, cat_embsizes[cat], input_length=1)(x)\n",
    "    x = Flatten()(x)\n",
    "    concat.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1= Input(shape=(4,),name='numerical')\n",
    "ins.append(inp1)\n",
    "d1 = Dense(128, activation='relu',kernel_initializer='glorot_normal')(inp1)\n",
    "concat.append(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "reg=tf.keras.regularizers.l2(0.001)\n",
    "x = concatenate(concat)\n",
    "x=BatchNormalization()(x)\n",
    "x= Dense(256,kernel_initializer='glorot_normal',activation='relu',kernel_regularizer=reg)(x)\n",
    "x= Dropout(0.6)(x)\n",
    "x= Dense(128,kernel_initializer='glorot_normal',activation='relu',kernel_regularizer=reg)(x)\n",
    "x= Dropout(0.5)(x)\n",
    "x= Dense(64,kernel_initializer='glorot_normal',activation='relu',kernel_regularizer=reg)(x)\n",
    "x= Dense(32,kernel_initializer='glorot_normal',activation='relu',kernel_regularizer=reg)(x)\n",
    "out= Dense(2,activation='softmax',kernel_initializer='glorot_normal',name='final')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=ins, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/51734992\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=[auroc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 300, 300)     14181900    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "teacher_prefix (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "school_state (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "project_grade_category (InputLa [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "project_subject_categories (Inp [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "project_subject_subcategories ( [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 300, 150)     270600      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 4)         32          teacher_prefix[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 26)        1378        school_state[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 3)         18          project_grade_category[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 26)        1378        project_subject_categories[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 50)        19450       project_subject_subcategories[0][\n",
      "__________________________________________________________________________________________________\n",
      "numerical (InputLayer)          [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 45000)        0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4)            0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 26)           0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 3)            0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 26)           0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 50)           0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          640         numerical[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 45237)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 45237)        180948      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          11580928    batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 32)           2080        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "final (Dense)                   (None, 2)            66          dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 26,280,570\n",
      "Trainable params: 12,008,196\n",
      "Non-trainable params: 14,272,374\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69918 samples, validate on 17480 samples\n",
      "Epoch 1/12\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "  256/69918 [..............................] - ETA: 15:55 - loss: 2.2042 - auroc: 0.4934WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (12.860288). Check your callbacks.\n",
      "69888/69918 [============================>.] - ETA: 0s - loss: 1.4004 - auroc: 0.6009\n",
      "Epoch 00001: saving model to ./model_save/weights-01.hdf5\n",
      "69918/69918 [==============================] - 155s 2ms/sample - loss: 1.4002 - auroc: 0.6023 - val_loss: 1.1505 - val_auroc: 0.6850\n",
      "Epoch 2/12\n",
      "69888/69918 [============================>.] - ETA: 0s - loss: 0.8705 - auroc: 0.7024\n",
      "Epoch 00002: saving model to ./model_save/weights-02.hdf5\n",
      "69918/69918 [==============================] - 128s 2ms/sample - loss: 0.8704 - auroc: 0.7023 - val_loss: 0.7859 - val_auroc: 0.7448\n",
      "Epoch 3/12\n",
      "69888/69918 [============================>.] - ETA: 0s - loss: 0.7043 - auroc: 0.7377\n",
      "Epoch 00003: saving model to ./model_save/weights-03.hdf5\n",
      "69918/69918 [==============================] - 128s 2ms/sample - loss: 0.7043 - auroc: 0.7377 - val_loss: 0.6716 - val_auroc: 0.7556\n",
      "Epoch 4/12\n",
      "69888/69918 [============================>.] - ETA: 0s - loss: 0.6479 - auroc: 0.7567\n",
      "Epoch 00004: saving model to ./model_save/weights-04.hdf5\n",
      "69918/69918 [==============================] - 129s 2ms/sample - loss: 0.6478 - auroc: 0.7575 - val_loss: 0.6186 - val_auroc: 0.7561\n",
      "Epoch 5/12\n",
      "69888/69918 [============================>.] - ETA: 0s - loss: 0.6142 - auroc: 0.7741\n",
      "Epoch 00005: saving model to ./model_save/weights-05.hdf5\n",
      "69918/69918 [==============================] - 128s 2ms/sample - loss: 0.6143 - auroc: 0.7734 - val_loss: 0.6099 - val_auroc: 0.7416\n",
      "Epoch 6/12\n",
      "69888/69918 [============================>.] - ETA: 0s - loss: 0.6050 - auroc: 0.7908\n",
      "Epoch 00006: saving model to ./model_save/weights-06.hdf5\n",
      "69918/69918 [==============================] - 129s 2ms/sample - loss: 0.6050 - auroc: 0.7908 - val_loss: 0.6006 - val_auroc: 0.7486\n",
      "Epoch 7/12\n",
      "69888/69918 [============================>.] - ETA: 0s - loss: 0.6039 - auroc: 0.8094\n",
      "Epoch 00007: saving model to ./model_save/weights-07.hdf5\n",
      "69918/69918 [==============================] - 130s 2ms/sample - loss: 0.6038 - auroc: 0.8099 - val_loss: 0.6265 - val_auroc: 0.7378\n",
      "Epoch 8/12\n",
      "69888/69918 [============================>.] - ETA: 0s - loss: 0.6193 - auroc: 0.8273\n",
      "Epoch 00008: saving model to ./model_save/weights-08.hdf5\n",
      "69918/69918 [==============================] - 129s 2ms/sample - loss: 0.6193 - auroc: 0.8276 - val_loss: 0.6593 - val_auroc: 0.7261\n",
      "Epoch 9/12\n",
      "69888/69918 [============================>.] - ETA: 0s - loss: 0.6258 - auroc: 0.8476\n",
      "Epoch 00009: saving model to ./model_save/weights-09.hdf5\n",
      "69918/69918 [==============================] - 130s 2ms/sample - loss: 0.6258 - auroc: 0.8474 - val_loss: 0.6933 - val_auroc: 0.7208\n",
      "Epoch 10/12\n",
      "69888/69918 [============================>.] - ETA: 0s - loss: 0.6414 - auroc: 0.8661\n",
      "Epoch 00010: saving model to ./model_save/weights-10.hdf5\n",
      "69918/69918 [==============================] - 129s 2ms/sample - loss: 0.6414 - auroc: 0.8660 - val_loss: 0.7355 - val_auroc: 0.7116\n",
      "Epoch 11/12\n",
      "69888/69918 [============================>.] - ETA: 0s - loss: 0.6488 - auroc: 0.8857\n",
      "Epoch 00011: saving model to ./model_save/weights-11.hdf5\n",
      "69918/69918 [==============================] - 132s 2ms/sample - loss: 0.6488 - auroc: 0.8856 - val_loss: 0.7870 - val_auroc: 0.7104\n",
      "Epoch 12/12\n",
      "69888/69918 [============================>.] - ETA: 0s - loss: 0.6500 - auroc: 0.9033\n",
      "Epoch 00012: saving model to ./model_save/weights-12.hdf5\n",
      "69918/69918 [==============================] - 130s 2ms/sample - loss: 0.6501 - auroc: 0.9031 - val_loss: 0.8123 - val_auroc: 0.6884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x219047cb808>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath=\"./model_save/weights-{epoch:02d}.hdf5\" \n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,patience=1, min_lr=0.002,verbose = 1)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_auc', verbose=1, mode='max')\n",
    "log_dir=\".\\logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True)\n",
    "model.fit([xtr_essay,xtr_Tprefix,xtr_state,xtr_grade,xtr_cat,xtr_subcat,xtr_num],y_train,epochs=12,         \n",
    "batch_size=256,verbose=1,\n",
    "validation_data=([xcv_essay,xcv_Tprefix,xcv_state,xcv_grade,xcv_cat,xcv_subcat,xcv_num],y_cv) ,callbacks=[tensorboard_callback,checkpoint,reduce_lr] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model\n",
    "\n",
    "#loading best model \n",
    "c={\"auroc\":auroc}\n",
    "model_tst_1=tf.keras.models.load_model(\"./model_save/weights-04.hdf5\", custom_objects=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=model_tst_1.evaluate([xte_essay,xte_Tprefix,xte_state,xte_grade,xte_cat,xte_subcat,xte_num],y_test,batch_size=256,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy of test data= 0.622900012183244\n",
      "auc-roc of test data= 0.7442839\n"
     ]
    }
   ],
   "source": [
    "print(\"cross entropy of test data=\",r[0])\n",
    "print(\"auc-roc of test data=\",r[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The auc-roc for test data is 0.744\n",
    "2. After around the 5th epoch, model starts to overfit and it continues\n",
    "3. The weights for thje final layer are between -0.5 to 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pics/model1_plot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pics/model1_hist.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
